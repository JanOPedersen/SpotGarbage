{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e6d389e96998f597cee0fc4b39534997436beb1ddafc62a05299fd5c50d2177c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we do data preparation. \n",
    "\n",
    "We start by installing the necessary packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "from pathlib import PurePath\r\n",
    "from math import cos, sin\r\n",
    "from glob import glob\r\n",
    "import csv\r\n",
    "import random\r\n",
    "from torch_snippets import torch, read, randint, Dataset, write, plt\r\n",
    "import numpy as np\r\n",
    "import matplotlib\r\n",
    "from tqdm import notebook\r\n",
    "from PoissonDiskSampling import PoissonDiskSampling\r\n",
    "from cropRotatedSquare import cropRotatedSquare\r\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "DATA_PATH = \"spotgarbage-GINI-master/spotgarbage/\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-02 08:13:01.307 | WARNING  | torch_snippets.torch_loader:<module>:233 - Not importing Lightning Report\n",
      "2021-09-02 08:13:03.820 | WARNING  | torch_snippets:<module>:13 - sklearn is not found. Skipping relevant imports from submodule `sklegos`\n",
      "Exception: No module named 'sklego'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by reading all the GINI files and building a map from file name to file location. We will need this map so that we can augment the 'garbage-queried-images.csv' file with file locations in addition to the bounding boxes it originally contained. Both the path and the folder name just beneath the actual image file is included. The folder name gives a kind of class label. The boolean at the end of each list tells if the image is garbage or non-garbage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "name2path = {}\r\n",
    "\r\n",
    "paths = [y for x in os.walk(DATA_PATH + \"garbage-queried-images\") for y in glob(os.path.join(x[0], \"*.*\"))]\r\n",
    "for path in paths:\r\n",
    "    pure = PurePath(path)\r\n",
    "    name2path[pure.name] = [path,pure.parts[-2],True]\r\n",
    "\r\n",
    "paths = [y for x in os.walk(DATA_PATH + \"non-garbage-queried-images\") for y in glob(os.path.join(x[0], '*.*'))]\r\n",
    "for path in paths:\r\n",
    "    pure = PurePath(path)\r\n",
    "    name2path[pure.name] = [path,pure.parts[-2],False] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now read the 'garbage-queried-images.csv' file and build a similar dictionary to the class label and bounding boxes. We skip the first row as this is where the column names are. There are no bounding boxes for non-garbage images, so we just put in None there"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "name2bounding = {}\r\n",
    "\r\n",
    "with open(DATA_PATH + \"garbage-queried-images.csv\", newline=\"\") as csvfile:\r\n",
    "    reader = csv.reader(csvfile, delimiter=\",\")\r\n",
    "    next(reader)\r\n",
    "    for row in reader:\r\n",
    "        if row[3] != \"\" and row[4] != \"\" and row[5] != \"\" and row[6] != \"\":\r\n",
    "            name2bounding[row[0]] = [row[1],row[3],row[4],row[5],row[6],True]\r\n",
    "\r\n",
    "with open(DATA_PATH + \"non-garbage-queried-images.csv\", newline='') as csvfile:\r\n",
    "    reader = csv.reader(csvfile, delimiter=\",\")\r\n",
    "    next(reader)\r\n",
    "    for row in reader:\r\n",
    "        name2bounding[row[0]] = [row[1],None,None,None,None,False]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will join name2path and name2bounding into one dictionary and save as CSV for later use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "name2info = {}\r\n",
    "for fileName in name2path:\r\n",
    "    if fileName in name2bounding:\r\n",
    "        name2info[fileName] = name2path[fileName] + name2bounding[fileName]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We store our new dictionary in a CSV file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "with open(DATA_PATH + \"image-meta-data.csv\", \"w\", newline=\"\") as csvfile:\r\n",
    "    writer = csv.writer(csvfile, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\r\n",
    "    writer.writerow([\"image\",\"path\",\"class\",\"is Garbage\",\"class\",\"bounding box\",\"is Garbage\"])\r\n",
    "    for fileName in name2info:\r\n",
    "        writer.writerow([fileName] + name2info[fileName])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And lets read the Dictionary again, so we can use it for making the DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "name2info = {}\r\n",
    "with open(DATA_PATH + \"image-meta-data.csv\", newline=\"\") as csvfile:\r\n",
    "    reader = csv.reader(csvfile, delimiter=\",\", quotechar='\"')\r\n",
    "    next(reader)\r\n",
    "    for row in reader:\r\n",
    "        name2info[row[0]] = row[1:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have enough structure to create our Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class GINIdata(Dataset):\r\n",
    "    def __init__(self, aug=None):\r\n",
    "        self.name2info = []\r\n",
    "        with open(DATA_PATH + \"image-meta-data.csv\", newline=\"\") as csvfile:\r\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\r\n",
    "            next(reader)\r\n",
    "            for row in reader:\r\n",
    "                name = row[0]\r\n",
    "                if  PurePath(name).suffix != \".gif\":\r\n",
    "                    self.name2info.append(row)\r\n",
    "        self.aug = aug\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.name2info)\r\n",
    "    def __getitem__(self, ix):\r\n",
    "        name         = self.name2info[ix][0]\r\n",
    "        path         = self.name2info[ix][1]\r\n",
    "        garbageClass = self.name2info[ix][2]\r\n",
    "        isGarbage    = (self.name2info[ix][3] == \"True\")\r\n",
    "        boundingBox  = [self.name2info[ix][5],self.name2info[ix][6],self.name2info[ix][7],self.name2info[ix][8]]\r\n",
    "        image        = read(path,1)\r\n",
    "        return image, boundingBox, garbageClass, isGarbage, name\r\n",
    "    def choose(self): return self[randint(len(self))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "gini = GINIdata()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have easy access to our images we will generate the patches as described in the paper. The method is as follows:\n",
    "\n",
    "a) Generate some Poisson disk samples with stride 9.1% of smaller image dimension\n",
    "\n",
    "b) For each sample (a square with length 10%, 20%, 40% or 70% smaller image dimension) generate a random rotation\n",
    "\n",
    "c) Find out if our sample fits into the image when the center of sample is the Poisson random disk center\n",
    "\n",
    "d) If it does not fit shift it so that it just does\n",
    "\n",
    "We create a utility function that takes an image dimensions (h,w), rotation angle (a), Poisson disk centre (x,y) and patch size (d) as inputs and generates an output that is modified if it does not fit into the image so that it just fits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def fitPatch(h, w, a, y, x, d):\r\n",
    "    # Find bounding box size\r\n",
    "    d1 = d * (np.cos(a) + np.sin(a))\r\n",
    "    \r\n",
    "    # correct x and y if needed\r\n",
    "    x = d1/2     if x - d1/2 < 0 else x\r\n",
    "    x = w - d1/2 if x + d1/2 > w else x\r\n",
    "    y = d1/2     if y - d1/2 < 0 else y\r\n",
    "    y = h - d1/2 if y + d1/2 > h else y\r\n",
    "\r\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: check that we use the boundingBox variable below and not the image itself, unless there is no bounding box"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not os.path.exists(DATA_PATH + \"patches\"):\r\n",
    "    os.mkdir(DATA_PATH + \"patches\")\r\n",
    "    # We have to seed both the numpy random generator and the one in the random module\r\n",
    "    np.random.seed(42)\r\n",
    "    random.seed(42)\r\n",
    "    patch_size_factors0 = [0.1, 0.2, 0.4, 0.7] # patch size in proportion to minimal image dimension\r\n",
    "    default_patch_num = [75, 19, 4, 2] # default number of patches for each size above\r\n",
    "    patch_size_factors = [] # for each patch, its relative size\r\n",
    "    N = 4 # number of random rotations per patch\r\n",
    "    for j in range(len(patch_size_factors0)):\r\n",
    "        patch_size_factors = patch_size_factors + [patch_size_factors0[j] for _ in range(default_patch_num[j])] \r\n",
    "   \r\n",
    "    for i in notebook.tnrange(len(gini), desc=\"patches\"):\r\n",
    "        image, boundingBox, garbageClass, isGarbage, name = gini[i]\r\n",
    "\r\n",
    "        # If the image is a garbage image we will need to crop it to its bounding box\r\n",
    "        if isGarbage:\r\n",
    "            boundingBox = [int(x) for x in boundingBox]\r\n",
    "            height, width = boundingBox[3] - boundingBox[1], boundingBox[2] - boundingBox[0]\r\n",
    "            image = image[boundingBox[1]:boundingBox[3], boundingBox[0]:boundingBox[2]] # crop image\r\n",
    "        else:\r\n",
    "            height, width = image.shape[0],image.shape[1]\r\n",
    "\r\n",
    "        smaller_dim = min(height, width)\r\n",
    "        patch_sizes = [x * smaller_dim for x in patch_size_factors] # for each patch, its absolute size\r\n",
    "        samples = PoissonDiskSampling(width, height, smaller_dim * 0.091)\r\n",
    "        nof_samples = min(100,len(samples))\r\n",
    "        patch_sizes = patch_sizes[0:nof_samples] \r\n",
    "        patch_num = default_patch_num # actual number of patches for each size in patch_size_factors0\r\n",
    "        patch_num[0] = default_patch_num[0] - (100 - nof_samples) # reduce the number of the smallest square patches\r\n",
    "        samples = random.sample(samples,nof_samples)\r\n",
    "        randomRotations = [random.uniform(0.0, np.pi / 2) for _ in range(4 * nof_samples)]\r\n",
    "        for j in range(nof_samples):\r\n",
    "            samples[j] = fitPatch(height, width, randomRotations[j], samples[j][1], samples[j][0], patch_sizes[j])\r\n",
    "\r\n",
    "        # calculate the corner points of each patch square and crop the image (OpenCV operations)\r\n",
    "        for j in range(N * nof_samples):\r\n",
    "            x, y = samples[j % N]\r\n",
    "            theta = randomRotations[j]\r\n",
    "            rot = -np.array([[cos(theta), -sin(theta)], [sin(theta), cos(theta)]])\r\n",
    "            corners = [\r\n",
    "                [  patch_sizes[j % N]/2,   patch_sizes[j % N]/2],\r\n",
    "                [  patch_sizes[j % N]/2, - patch_sizes[j % N]/2],\r\n",
    "                [- patch_sizes[j % N]/2,   patch_sizes[j % N]/2],\r\n",
    "                [- patch_sizes[j % N]/2, - patch_sizes[j % N]/2]]\r\n",
    "\r\n",
    "            rotated = [[np.dot(rot, r) + [x,y]] for r in corners]\r\n",
    "            rotated = np.array([[[int(c[0]),int(c[1])] for c in row] for row in rotated])\r\n",
    "\r\n",
    "            # The rotated array contains the corners of the cropping square, so now we\r\n",
    "            # do the cropping\r\n",
    "            cropped = cropRotatedSquare(image, rotated)\r\n",
    "            write(cropped,DATA_PATH + f\"patches/{name}_{j}.png\")\r\n",
    "\r\n",
    "print(\"done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us visualize our patches so we can get an understanding of both the distribution of Poisson disks and validate that they are all within the image boundary. We calculate the rectangles both directly from the angles and from the polygon vectors of 4 corner points that are generated inside the loop for patch generation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib.patches import Rectangle\r\n",
    "showSamples = np.transpose(np.array(samples))\r\n",
    "fig, ax = plt.subplots()\r\n",
    "\r\n",
    "rect_img =  Rectangle((0, 0),\r\n",
    "                        width, height,\r\n",
    "                        fc =\"none\", \r\n",
    "                        ec =\"r\",\r\n",
    "                        lw = 1)\r\n",
    "\r\n",
    "for i in range(len(samples)):\r\n",
    "    x, y = samples[i]\r\n",
    "\r\n",
    "    t_start = ax.transData\r\n",
    "    t2 = matplotlib.transforms.Affine2D().rotate_deg_around(x,y, randomRotations[i] * 180 / np.pi)\r\n",
    "    t =  t2 + t_start \r\n",
    "\r\n",
    "    rect =  Rectangle((x - patch_sizes[i]/2, y - patch_sizes[i]/2),\r\n",
    "                        patch_sizes[i], patch_sizes[i],\r\n",
    "                        fc =\"none\", \r\n",
    "                        ec =\"g\",\r\n",
    "                        lw = 1, transform = t)\r\n",
    "\r\n",
    "    ax.add_patch( rect )\r\n",
    "   \r\n",
    "ax.add_patch( rect_img )\r\n",
    "ax.axis(\"equal\")\r\n",
    "plt.xlim([-width/10, width + width/10])\r\n",
    "plt.ylim([-height/10, height + height/10])\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The file names we created when generating the patch files got wrong, so let's rename them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with os.scandir(DATA_PATH + \"patches\") as it:\r\n",
    "    for entry in it:\r\n",
    "        NAME = str(PurePath(entry.name))\r\n",
    "        parts = str(NAME).split(sep = '.')\r\n",
    "        if len(parts) == 3:\r\n",
    "            newName = parts[0] + parts[1][3:] + \".\" + parts[2]\r\n",
    "            os.rename(DATA_PATH + \"patches/\" + NAME,DATA_PATH + \"patches/\" + newName)\r\n",
    "       \r\n",
    "print(\"done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above step more than 745.000 patch files where produced. We would like to delete randomly 245.000 of them leaving some 500.000 remaining. These should be divided into 250.000 garbage and 250.000 non-garbage patches. First lets count how many of each type we have."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "GarbageNames = set()\r\n",
    "paths = [y for x in os.walk(DATA_PATH + \"garbage-queried-images\") for y in glob(os.path.join(x[0], '*.*'))]\r\n",
    "for path in paths:\r\n",
    "    pure = PurePath(path)\r\n",
    "    GarbageNames.add(str(pure.name)[:-4])\r\n",
    "\r\n",
    "NonGarbageNames = set()\r\n",
    "paths = [y for x in os.walk(DATA_PATH + \"non-garbage-queried-images\") for y in glob(os.path.join(x[0], '*.*'))]\r\n",
    "for path in paths:\r\n",
    "    pure = PurePath(path)\r\n",
    "    NonGarbageNames.add(str(pure.name)[:-4])\r\n",
    "\r\n",
    "print(len(GarbageNames))\r\n",
    "print(len(NonGarbageNames))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "garbageCounter = 0\r\n",
    "NonGarbageCounter = 0\r\n",
    "garbageList = []\r\n",
    "nonGarbageList = []\r\n",
    "\r\n",
    "with os.scandir(DATA_PATH + \"patches\") as it:\r\n",
    "    for entry in it:\r\n",
    "        NAME = str(PurePath(entry.name))\r\n",
    "        parts = str(NAME).split(sep = '_')\r\n",
    "        if parts[0] in GarbageNames:\r\n",
    "            garbageCounter += 1\r\n",
    "            garbageList.append(NAME)\r\n",
    "        if parts[0] in NonGarbageNames:\r\n",
    "            NonGarbageCounter += 1\r\n",
    "            nonGarbageList.append(NAME)\r\n",
    "        \r\n",
    "print(garbageCounter) # 131272\r\n",
    "print(NonGarbageCounter) # 604652\r\n",
    "print(len(garbageList))\r\n",
    "print(len(nonGarbageList))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "deleteNonGarbageList = random.sample(nonGarbageList,len(nonGarbageList) - 130000)\r\n",
    "deleteGarbageList = random.sample(garbageList,len(garbageList) - 130000)\r\n",
    "print(len(deleteNonGarbageList))\r\n",
    "print(len(deleteGarbageList))\r\n",
    "deleteNonGarbageSet = set(deleteNonGarbageList)\r\n",
    "deleteGarbageSet = set(deleteGarbageList)\r\n",
    "print(len(deleteNonGarbageSet))\r\n",
    "print(len(deleteGarbageSet))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have our two lists of items to delete, we carry on with the deleting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with os.scandir(DATA_PATH + \"patches\") as it:\r\n",
    "    for entry in it:\r\n",
    "        NAME = str(PurePath(entry.name))\r\n",
    "        \r\n",
    "        if NAME in deleteNonGarbageSet:\r\n",
    "            os.remove(DATA_PATH + \"patches/\" + NAME)\r\n",
    "\r\n",
    "        if NAME in deleteGarbageSet:\r\n",
    "            os.remove(DATA_PATH + \"patches/\" + NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find out if all files are indeed PNG files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 0\r\n",
    "j = 0\r\n",
    "with os.scandir(DATA_PATH + \"patches\") as it:\r\n",
    "    for entry in it:\r\n",
    "        j += 1\r\n",
    "        NAME = str(PurePath(entry.name))\r\n",
    "        if NAME.endswith(\".png\"):\r\n",
    "            i += 1\r\n",
    "\r\n",
    "print(i,j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For generating the 5 stratified folds in the paper we will need a simple table like structure for the patces, with patch name and class label. We store this in a simple CSV file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(DATA_PATH + \"train.csv\", \"w\", newline=\"\") as csvfile, os.scandir(DATA_PATH + \"patches\") as it:\r\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\r\n",
    "    writer.writerow([\"name\",\"class\"])\r\n",
    "    for entry in it:\r\n",
    "        name = str(PurePath(entry.name))\r\n",
    "        parts = str(name).split(sep = \"_\")\r\n",
    "        if parts[0] in GarbageNames:\r\n",
    "            writer.writerow([name,\"garbage\"])\r\n",
    "        if parts[0] in NonGarbageNames:\r\n",
    "            writer.writerow([name,\"nongarbage\"])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}